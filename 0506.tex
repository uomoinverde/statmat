
\chapter{Sufficienza}

\section{Introduzione}

\begin{dfn}[\textsc{mvue}]
  Sia \((X_1,\dotsc,X_n)\) un campione casuale da una distribuzione di 
  densità \(f_X(\mathbf{x};\theta)\), \(\theta \in \Theta\). Sia
  \(Y = u(X_1,\dotsc,X_n)\) uno stimatore del parametro \(\theta\). Si dice
  \(Y\) essere \emph{stimatore a minima varianza nella classe dei non distorti} per \(\theta\) se
  \begin{enumerate}
    \item \(Y\) è uno stimatore non distorto di \(\theta\), ovvero
          \(\mathbb{E}(Y) = \theta\) e
    \item per ogni stimatore non distorto \(W\) di \(\theta\),
          \(\var(Y) \le \var(W)\).
  \end{enumerate}
\end{dfn}

Rinfreschiamo il concetto di distribuzione condizionata:
\begin{dfn}[Distribuzione condizionata]
Siano \(X\), \(Y\) variabili casuali. Sia \(x \in \mathcal{S}_X\).
Per ogni punto \(y \in \mathcal{S}_Y\), definiamo:
\begin{description}
  \item[\textsc{caso discreto}] la \emph{pmf condizionata}
  \begin{equation}
    p_{Y\mid X}(y \mid x) = \frac{p_{X,Y}(x,y)}{p_X(x)}
  \end{equation}
  dove \(p_{X,Y}\) è la funzione di massa congiunta di \(X\),\(Y\) e \(p_X\) rappresenta la funzione di massa probabilistica marginale di \(X\).
  \item[\textsc{caso continuo}] la \emph{pdf condizionata}
  \begin{equation}
    f_{Y\mid X}(y \mid x) = \frac{f_{X,Y}(x,y)}{f_X(x)}
  \end{equation}
  dove \(f_{X,Y}\) è la densità congiunta delle due variabili e \(f_X\) è la densità marginale di \(X\).
\end{description}
\end{dfn}

\noindent \textbf{NB:} $f_{Y|X}(y | X=x) = f_Y(y)$ per ogni y se e solo se X e Y sono indipendenti.

\paragraph{Cosa rappresenta la sufficienza?}
La sufficienza di una statistica definisce formalmente la capacità di tale funzione di rappresentare in maniera \emph{sintetica} l'informazione contenuta nel campione, senza perdita di \emph{informazione rilevante}.
Intuitivamente si può pensare alla proprietà di conservazione dell'informazione rilevante in questo modo: qualsiasi altra statistica, calcolata a partire dallo stesso campione, non porta più informazioni rispetto a \(\theta\) di quante ne abbia già portato la statistica sufficiente.
Per questo, banalmente, l'intero campione è sicuramente una statistica sufficiente (molto poco sintetica, ma a volte non c'è di meglio).

\begin{dfn}[Sufficienza]
  Sia \((X_1,\dotsc,X_n)\) un campione casuale da una distribuzione avente funzione densità/massa probabilistica \(f_X(x;\theta)\). Sia inoltre \(T_n(X_1,\dotsc,X_n)\) uno stimatore di \(\theta\) avente funzione di densità o massa probabilistica \(f_{T_n}(t; \theta)\).
  La statistica \(T_n\) viene detta \emph{sufficiente} per \(\theta\) se la distribuzione di \(X_1,\dotsc,X_n\) condizionata a \(T_n(x_1,\dotsc,x_n) = t_n\)
  \begin{equation*}
    H(x_1,\dotsc,x_n) = \frac{\prod_{i=1}^n f_{X}(x_i;\theta)}{f_{T_n}(t_n;\theta)}
  \end{equation*}
  è una funzione non dipendente da \(\theta\).
\end{dfn}

Abbiamo quindi formalizzato le richieste fatte sopra: il fatto che la distribuzione condizionata (del vettore allo stimatore) non dipenda da \(\theta\) implica di fatto l'impossibilità di perdere informazioni rilevanti su \(\theta\) stesso, e cioè, lo stimatore \(T_n\) contiene tutte le informazioni necessarie riguardanti \(\theta\) per fare inferenza sul parametro incognito.
In altre parole, \textit{le informazioni contenute in \(T_n\)} riguardo a \(\theta\) \textit{sono le stesse contenute nell'intero campione}.

Vedremo che la sufficienza può essere verificata usando il teorema di fattorizzazione di Neyman, che di fatto fornisce una caratterizzazione 'più comoda' da verificare (rispetto all'uso brutale della definizione) per assicurarsi che una data statistica abbia la proprietà desiderata.

\subsection{Esempi} 


\begin{ese}
  Sia \(X = (X_1,\dotsc,X_n) \sim b(1,p)\).
  Allora, \(T_n(X)\coloneqq \sum_{i=1}^n X_i\) è una statistica sufficiente per \(p\). Infatti, considerando la massa condizionata di \(X\) rispetto a \(T_n\), si verifica che
  \begin{equation*}
    f_{X|T_n}(\mathbf{x},t_n) = \frac{f_{X,T_n}(\mathbf{x},t_n)}{f_{T_n}(t_n)} =
    \frac{p^{t_n}(1-p)^{n-t_n}}{\binom{n}{t_n}p^{t_n}(1-p)^{n-t_n}} =
    \frac{1}{\binom{n}{t_n}}.
  \end{equation*}
  Notiamo che \(\binom{n}{t_n}^{-1}\) è indipendente da \(p\), per cui \(T_n\) è statistica sufficiente per \(p\).
\end{ese}

\begin{ese}
  Sia \(X = (X_1,\dotsc,X_n) \sim \mathcal{G}(\alpha=2,\beta)\). Allora, \(T_n(X)\coloneqq \sum_{i=1}^n X_i\) è una statistica sufficiente per \(\beta\).
  Infatti, si ha innanzi tutto che \(T_n(X)\sim \mathcal{G}(2n,\beta)\) grazie all'indipendenza delle osservazioni e alla proprietà di riproducibilità di \(G\). Allora
  \begin{equation*}\begin{split}
    f_{X|T_n}(\mathbf{x},t_n) &= \frac{f_{X,T_n}(\mathbf{x},t_n)}{f_{T_n}(t_n)} = \frac{f_{X}(\mathbf{x})}{f_{T_n}(t_n)} \\
    &= \frac{\prod_{i=1}^n \frac{1}{\Gamma(2)\beta^2}x_i^{2-1}e^{-\frac{1}{\beta}x_i}}{\frac{1}{\Gamma(2n)\beta^{2n}}t_n^{2n-1}e^{-\frac{1}{\beta}t_n}} \\
    &=\frac{\Gamma(2n)\prod_{i=1}^n x_i}{{(2!)}^n t_n^{2n-1}},
  \end{split}
\end{equation*}
  da cui si evince la sufficienza di \(T_n\).
\end{ese}

\begin{ese}
Sia $\vec{X}\coloneqq (X_1,\ldots,X_n)$ un campione casuale avente funzione di densità $f_X(x,\theta)\coloneqq e^{-(x-\theta)}\mathbbm{1}_{(\theta, +\infty)}(x)$, $\theta>0$. 
Allora la statistica d'ordine $X_{(1)}$ è sufficiente per \(\theta\). Infatti: 
$$f_{X_{(1)}}(x,\theta)=ne^{-n(x-\theta)}$$ ed otteniamo subito che $$f_{\vec{X} | X_{(1)}}(\vec{x},x_{(1)},\theta)=\frac{f_{\vec{X}}(\vec{x},\theta)}{f_{X_{(1)}}(x_{(1)})}=\frac{\prod_{i=1}^n e^{-(x_i-\theta)}\mathbbm{1}_{(\theta,+\infty)}(x_i)}{ne^{-n(x_{(1)}-\theta)}\mathbbm{1}_{(\theta,+\infty)}(x_{(1)})}=\frac{e^{-\sum_{i=1}^n x_i}}{ne^{-nx_{(1)}}}$$ che non dipende dal parametro \(\theta\).
\end{ese}

\begin{thm}[Fattorizzazione di Neyman] Sia \(X = (X_1,\dotsc,X_n)\) un campione casuale da una popolazione di densità/massa \(f_X(x,\theta)\).
La statistica \(T_n=T_n(X_1,\dotsc,X_n)\) è statistica sufficiente per \(\theta\) se e solo se esistono due funzioni non negative \(g\),\(h\) con le seguenti richieste:
\begin{enumerate}
\item \(h=h(\mathbf{x})\) sia indipendente da \(\theta{}\) e
\item \(g=g(\theta, t_n)\) dipenda da \(\theta\) e dalla determinazione del campione \(\mathbf{x}\) solo attraverso \(t_n\),
\end{enumerate}
e tali che, per ogni \(\mathbf{x}_0 \in \mathfrak{X}\), \(\theta_0 \in \Theta\),
la funzione di verosimiglianza possa essere scritta come
\begin{equation}
  L(\theta_0,\mathbf{x}_0)=h(\mathbf{x}_0)\cdot g(\theta_0, t_n(\mathbf{x}_0)).
\end{equation}
\end{thm}

\begin{proof}
  Assumiamo che \(T_n\) sia statistica sufficiente per \(\theta{}\).
  Per definizione di densità condizionata e indipendenza delle osservazioni di un campione,
  \begin{equation*}
    f_{X \mid T_n}(\mathbf{x},t_n,\theta) = 
    \frac{\prod_{i=1}^n f_X(x_i;\theta)}{f_{T_n}(t_n,\theta)}
    \implies
    L(\theta; \mathbf{x}) = f_{X \mid T_n}(\mathbf{x},t_n,\theta)
    \cdot f_{T_n}(t_n,\theta);
  \end{equation*}
  inoltre, dalla sufficienza di \(T_n\) si ricava l'indipendenza della densità condizionata rispetto a \(\theta{}\). Dunque, ponendo
  \begin{align*}
    h(\mathbf{x}) &:= f_{X \mid T_n}(\mathbf{x},t_n) \\
    g(\theta,t_n) &:= f_{T_n}(t_n,\theta)
  \end{align*}
  si ottiene la fattorizzazione \(L(\theta;\mathbf{x}) =
  h(\mathbf{x})g(\theta,t_n)\).

  Assumiamo ora che, con le condizioni poste dal teorema, che sia possibile fattorizzare \(L(\theta;\mathbf{x})\) nel modo sopra descritto. Dimostriamo il caso in cui il campione sia composto da variabili casuali discrete.
  Definiamo l'insieme di livello
  \begin{align*}
    A_n :&= \lbrace \mathbf{x} \in \mathcal{X} \colon T_n(\mathbf{x})
         = t_n \rbrace.
    \intertext{e osserviamo che}
    f_{T_n}(t_n) &= \sum_{\mathbf{x} \in A_n} L(\theta,\mathbf{x}) =
    \sum_{\mathbf{x}\in A_n}h(\mathbf{x})g(\theta,t_n) =
    g(\theta,t_n)\sum_{\mathbf{x}\in A_n}h(\mathbf{x}).
  \end{align*}
  Ricorrendo alla definizione di densità condizionata, si ottiene la relazione
  \begin{equation*}
    f_{X \mid T_n}(\mathbf{x},t_n) =
    \frac{L(\theta,\mathbf{x})}{f_{T_n}(t_n)} =
    \frac{h(\mathbf{x})g(\theta,t_n)}{g(\theta,t_n)\sum_{\mathbf{x}\in A_n}h(\mathbf{x})} =
    \frac{h(\mathbf{x})}{\sum_{\mathbf{x}\in A_n}h(\mathbf{x})}.
  \end{equation*}
  Non essendoci dipendenza dal parametro \(\theta{}\), possiamo concludere che \(T_n\) è statistica sufficiente per esso.
  La dimostrazione per il caso continuo è analoga.
\end{proof}
