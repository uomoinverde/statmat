\subsection{Statistiche d'ordine}
Lezioni 08 e 11 Marzo, ultima modifica 21/03, Scritte da: Marco Peruzzetto

\begin{definizione} Sia $\left(X_1,\ldots ,X_n\right)$ un campione casuale con distribuzione $F_X\left(x,\theta\right)$, densità $f_X\left(x,\theta\right)$ e supporto $\supp\{X\}\coloneqq (a,b)\subset\mathbb{R}$ ove $X\in \{X_1,\ldots,X_n\}$ e $-\infty\leq a<b\leq +\infty$. Definiamo ricorsivamente le seguenti variabili casuali:
\begin{itemize}
\item $X_{(1)}\coloneqq \min\left(\{X_1,\ldots,X_n\}\right)$;
\item $X_{(i)}\coloneqq \min\left(\{X_1,\ldots,X_n\}\setminus\{X_{(1)},\ldots,X_{(i-1)}\}\right)$ $\forall 1<i\leq n$. 
\end{itemize}
Chiameremo allora $X_{(i)}$ la $i$-esima \textit{Statistica d'Ordine} del campione.
\end{definizione}

\textit{Osservazione:} La statistica d'ordine consiste semplicemente nel vettore per il quale le variabili casuali vengono appunto ordinate, in base al valore che assumono in un determinato punto del loro dominio comune, in ordine crescente. In particolare $X_{(i)}$ sarà l' $i$-esima variabile più piccola. Naturalmente, se il campione ha lunghezza $n$, allora $X_{(n)}=\max\left(\{X_1,\ldots,X_n\}\right)$. 
Osserviamo che la funzione $\left(X_1,\ldots,X_n\right)\longmapsto \left(X_{(1)},\ldots,X_{(n)}\right)$ è essa stessa una Statistica.
\begin{teo}\label{dist:ordine}
Sia $\left(X_1,\ldots,X_n\right)$ un campione casuale come sopra. Allora si ottiene $\forall$ $1\leq m\leq n$ che la densità dell' $m$-esima statistica d'ordine è data da: 
\begin{displaymath}
f_{X_{(m)}}\left(x,\theta\right)=\frac{n!}{(m-1)!(n-m)!}f_X\left(x,\theta\right)\cdot F_X\left(x,\theta\right)^{m-1}\cdot \big(1-F_X\left(x,\theta\right)\big)^{n-m}
\end{displaymath}
\end{teo}
Daremo due dimostrazioni, la seconda più bella della prima.
\begin{proof} (a cura di Marco Perruzzetto) Innanzi tutto si ha che il supporto $(a,b)$ può essere partizionato in $n$ parti, per cui evidentemente si ha: 
\footnotesize{
\begin{displaymath}
f_{(X_{(1)},\ldots,X_{(n)})}(x_{(1)},\ldots,x_{(n)},\theta)=
\left\{
\begin{array}{lr}
n!\prod_{i=1}^n f_{X}(x_{(i)},\theta) & \mbox{se } a<x_{(1)}<x_{(2)}<\ldots <x_{(n)}<b \\
0 & \mbox{altrimenti.}
\end{array}
\right.
\end{displaymath}
} \normalsize{ove la produttoria è giustificata dal fatto che le variabili sono tutte indipendenti e che devono essere ciascuna minore dell'altra per l'ordinamento assegnato; il coefficiente fattoriale è presente poiché le $n$ parti dell'intervallo $(a,b)$ possono essere assegnate alle $n$ variabili in tale numero di modi, dato che ciascuna $X_i$ $\forall 1\leq i\leq n$ ha la stessa distribuzione.  \\ Adesso per trovare la distribuzione di ciascuna $X_{(m)}$ sarà dunque sufficiente integrare $f_{(X_{(1)},\ldots,X_{(n)})}$ nei domini possibili di tutte le altre funzioni di distribuzione di ciascuna $X_{(i)}$ con $i\neq m$. In particolare, ciascuna $f_{X_{(i)}}$ per $i<m$ dovrà assumere a piacere valori necessariamente inferiori a $f_{X_{(m)}}$, viceversa ogni $f_{X_{(i)}}$ per $i>m$ dovrà assumere valori obbligatoriamente superiori a quelli di $f_{X_{(m)}}$ in ogni punto. Ricordando allora che possiamo scrivere la distribuzione come $\int_a^x f_X(\theta,t)dt=F_X(\theta,x)$ essendo la densità la derivata della funzione di distribuzione, otterremo quindi che $\forall a<x_{(m)}<b$ la distribuzione sarà data da: \\\\ $f_{X(m)}(x_{(m)},\theta)=$	}
\small{
\begin{eqnarray*}
&=&\int_a^{x_{(2)}} dx_{(1)}\cdots\int_a^{x_{(m)}} dx_{(m-1)}\int_{x_{(m)}}^b dx_{(m+1)}\cdots\int_{x_{(n-1)}}^b dx_{(n)}f_{(X_{(1)},\ldots,X_{(n)})}(x_{(1)},\ldots,x_{(n)},\theta)\\
&=&\int_a^{x_{(2)}} dx_{(1)}\cdots\int_a^{x_{(m)}} dx_{(m-1)}\int_{x_{(m)}}^b dx_{(m+1)}\cdots\int_{x_{(n-1)}}^b dx_{(n)} n!\prod_{i=1}^n f_{X}(x_{(i)},\theta) \\
&=& f_{X}(x_{(m)})\int_a^{x_{(2)}} dx_{(1)}\cdots\int_a^{x_{(m)}} dx_{(m-1)}\int_{x_{(m)}}^b dx_{(m+1)}\cdots\int_{x_{(n-1)}}^b dx_{(n)} n!\prod_{i=1,i\neq m}^n f_{X}(x_{(i)},\theta) \\
&=& \frac{n!}{(m-1)!(n-m)!}f_X\left(x,\theta\right)\cdot F_X\left(x,\theta\right)^{m-1}\cdot \big(1-F_X\left(x,\theta\right)\big)^{n-m}, 
\end{eqnarray*}
}
\normalsize{dove è stato usato il fatto che $\int_a^b F_X^\alpha(\theta, t)f_X(\theta,t)dt=\frac{F_X^{\alpha+1}}{\alpha+1}$, $\forall \alpha\neq -1$.}
\end{proof} 

\begin{proof}
Sia $\Omega$ il dominio comune del campione casuale. Definiamo per $x\in \mathbb{R}$ la nuova variabile casuale $Y_x$ come:
\begin{align*}
\Omega &\longrightarrow  \{0,\ldots,n\} \\
Y_x(\omega)  &\coloneqq   \sum_{i=1}^n \mathbbm{1}_{\{X_i(\omega)\leq x\}}(\omega)=\#\big\{i\in\{1,\ldots,n\}: X_i\leq x\big\},
\end{align*}
funzione che, per così dire, ``conta'' il numero di variabili casuali $X_i$ che non superano $x$. Si vede immediatamente che $\forall 1\leq m\leq n$, si ha la distribuzione 
\begin{eqnarray*}
F_{X_{(m)}}(\theta,x) &=& \mathbb{P}[X_{(m)} \leq x] = \mathbb{P}[\text{almeno } X_{(1)},...,X_{(m)} \text{ stanno sotto $x$}] = \\
&=& \mathbb{P}[Y_x\geq m] = \sum_{k=m}^n \mathbb{P}[Y_x=k] = \\
&=& \sum_{k=m}^n \binom{n}{k}F_X^k(\theta,x)\big(1-F_X(\theta,x)\big)^{n-k}.
\end{eqnarray*}
Come nella prima dimostrazione usiamo il fatto che la densità si può vedere come derivata della funzione di ripartizione. Ne segue che per calcolare la densità sarà sufficiente calcolare la derivata in ciascun punto $x$ della distribuzione appena trovata. In particolare si potrà vedere che coesisteranno il termine che vogliamo ottenere con altre due sommatorie, che tuttavia si elidono l'una con l'altra lasciando quindi la relazione espressa dal teorema. Si ha infatti che: 
\\
\small{
\begin{equation*}\label{e:barwq}\begin{split}
f_{(m)}(\theta,x)&=\frac{\partial}{\partial x}F_{X_{(m)}}(\theta,x)= \\
&=\sum_{k=m}^n \binom{n}{k}\cdot f_{X}(\theta,x) \bigg\{k F_X^{k-1}(\theta,x) \big(1-F_X(\theta,x)\big)^{n-k}-(n-k) F_X^{k}(\theta,x)\big(1-F_X(\theta,x)\big)^{n-k-1}\bigg\} \\
&=m\binom{n}{m}\cdot f_{X}(\theta,x)F_X^{m-1}(\theta,x) \big(1-F_X(\theta,x)\big)^{n-m} + \\ &\quad \sum_{k=m+1}^n k\binom{n}{k} f_{X}(\theta,x) F_X^{k-1}(\theta,x) \big(1-F_X(\theta,x)\big)^{n-k}- \\ &\quad\sum_{k=m}^{n-1} (n-k)\binom{n}{k} f_{X}(\theta,x) F_X^{k}(\theta,x)\big(1-F_X(\theta,x)\big)^{n-k-1} \\
&=\frac{n!}{(m-1)!(n-k)!}\cdot f_{X}(\theta,x)F_X^{m-1}(\theta,x) \big(1-F_X(\theta,x)\big)^{n-m} + \\
&\quad \sum_{j=m}^{n-1} (j+1)\binom{n}{j+1} f_{X}(\theta,x) F_X^{j}(\theta,x) \big(1-F_X(\theta,x)\big)^{n-j-1}- \\
& \quad \sum_{k=m}^{n-1} (n-k)\binom{n}{k} f_{X}(\theta,x) F_X^{k}(\theta,x)\big(1-F_X(\theta,x)\big)^{n-k-1} \\
&=\frac{n!}{(m-1)!(n-k)!}\cdot f_{X}(\theta,x)F_X^{m-1}(\theta,x) \big(1-F_X(\theta,x)\big)^{n-m} + \\
&\quad \sum_{j=m}^{n-1} \frac{n!}{j!(n-j-1)!} f_{X}(\theta,x) F_X^{j}(\theta,x) \big(1-F_X(\theta,x)\big)^{n-j-1}- \\
& \quad \sum_{k=m}^{n-1} \frac{n!}{k!(n-k-1)!} f_{X}(\theta,x) F_X^{k}(\theta,x)\big(1-F_X(\theta,x)\big)^{n-k-1} \\
&=\frac{n!}{(m-1)!(n-k)!}\cdot f_{X}(\theta,x)F_X^{m-1}(\theta,x) \big(1-F_X(\theta,x)\big)^{n-m}.
\end{split}\end{equation*}
}
\end{proof}
\begin{dfn}
  Dato un campione casuale \((X_1,\dotsc,X_n)\), definiamo le seguenti 
  variabili:
  \begin{itemize}
    \item il \emph{range} campionario o \emph{misura di dispersione}, \(X_{(n)}-X_{(1)}\);
    \item la \emph{misura di centralità}, \(\frac12(X_{(1)}+X_{(n)})\);  

  \item 
  $$
  \left.
  \begin{array}{rl}
  \forall n \mbox{ pari} & \frac{X_{(\frac{n}{2})}+X_{(\frac{n}{2}+1)}}{2} \\
  \forall n \mbox{ dispari} & X_{(\frac{n+1}{2})}
  \end{array}
  \right\} \mbox{dette ciascuna }\textit{Mediana campionaria};
  $$
  \item Sia $\frac{1}{2(n+1)}<p<1-\frac{1}{2(n+1)}$, che possiamo in ogni caso pensare come $0<p<1$ per $n$ molto grande. A questo punto possiamo definire l'intero $k_p\coloneqq \big\lfloor p(n+1) \big\rfloor + \big\lfloor 2\big(p(n+1)- 2 \lfloor p(n+1) \rfloor \big) \big\rfloor$, che risulta essere così ben definito in quanto compreso tra 1 e $n$ e restituisce l'approssimazione all'intero più vicino al variare di $p$ del reale $p(n+1)$. 
  \item A questo punto, se scegliamo $\xi_p\in F_X^{-1}(p)$, chiameremo $\xi_p$ \textit{Quantile di popolazione} di ordine $p$. In seguito troveremo utile stimare tale valore. Perciò introduciamo la variabile casuale ad esso collegata $X_{(k_p)}$, detta \textit{Quantile campionario} di ordine $p$. Se $p=\frac{i}{m}$, allora $X_{(k_p)}$ è detta anche $i$-esimo $m$-ile campionario. In particolare con $Q_1$ e $Q_3$ si indicano rispettivamente il primo e il terzo quartile.\\
  Intuitivamente, $X_{(k_p)}$ mi dà la v.c. che sta al $k_p$-esimo posto, ovvero al $p(n+1)$-esimo posto (se $p(n+1)$ è intero). Ad esempio, se $p=\frac{1}{3}$, $X_{(k_{\frac{1}{3}})}$ è la v.c. che nel vettore ordinato sta alla posizione $\frac{n+1}{3}$.
  \item Le variabili $LF\coloneqq Q_1-h$ e $UF\coloneqq h+Q_3$, ove $h\coloneqq \frac{3}{2}(Q_3-Q_1)$ sono dette rispettivamente \textit{Lower} e \textit{Upper Fence}. 
  \end{itemize}
\end{dfn}
\textbf{Definizione.} 
\textit{Osservazione:} Osserviamo che più la misura di centralità si discosta dalla mediana, più vi è asimmetria nella funzione di densità $f$ (\textit{i.e.:} una funzione di distribuzione è simmetrica $:\Longleftrightarrow$ $\exists x_0\in \mathbb{R}:f(x_0+x)=f(x_0-x), \forall x\in \dom(f).$) Inoltre, ponendo che la funzione di ripartizione sia iniettiva e la funzione di densità sia simmetrica, si vede immediatamente che la media di popolazione, ovvero il quantile di popolazione di ordine $p=\frac{1}{2}$ coincide con il valore di aspettazione della variabile casuale, il quale a sua volta deve coincidere con $x_0$. \\ \\
Dato un campione casuale di parametro $\theta\in \mathbb{R}$ fissato, sappiamo che una qualsiasi funzione di statistiche su tali variabili è, proprio per definizione, uno stimatore del parametro $\theta$. L'esistenza di un'infinità non numerabile di stimatori è sicuramente un problema da ovviare in merito alla scelta tra essi di uno stimatore che effettivamente permetta di stimare il più correttamente possibile il parametro $\theta$. Cercheremo dunque di individuare alcune proprietà che possano effettivamente giustificare la scelta di un determinato stimatore, affinché esso risulti il più possibile affidabile.\\ \\
\textbf{Definizione.} Sia $(X_1\ldots,X_n)$ un campione di parametro $\theta$ e $T_n(X_1,\ldots,X_n)$ uno stimatore. La funzione $B_{\theta}[T_n(X_1,\ldots,X_n)]\coloneqq \mathbb{E}_{\theta}[T_n(X_1,\ldots,X_n)]-\theta$ si dice \textit{distorsione} di $T_n$ (nota: con $\mathbb{E}_{\theta}$ formalmente intendiamo semplicemente $\mathbb{E}$). In particolare $T_n$ si dirà \textit{non distorto} se e solo se la sua distorsione è nulla $\forall \theta\in \mathbb{R}$ (nel senso che uno stimatore -e.g. la media campionaria- non può stimare bene solo "alcune" medie, ma qualsiasi media reale, ad esempio la media di qualsiasi normale centrata in qualsiasi punto). Altrimenti si dice \textit{distorto.} Se infine si ottiene che $\lim_{n\rightarrow +\infty} B_{\theta}[T_n(X_1,\ldots,X_n)]=0$, $T_n$ si dice \textit{asintoticamente non distorto}.
\\ \\
\noindent \textbf{Esempio} Sia $(X_{(1)},\ldots,X_{(n)})$ un campione casuale con distribuzione simmetrica (senza perdita di generalità, la assumiamo simmetrica rispetto all'origine) e scegliamo come stimatore $T_n$ proprio la mediana campionaria. È chiaro innanzi tutto che essa in generale gode delle seguenti due proprietà: \begin{itemize}
\item $\forall b\in \mathbb{R}, $ $T_n(X_1+b,\ldots,X_n+b)=T_n(X_1,\ldots,X_n)+b$;
\item $T_n(-X_ 1,\ldots,-X_n)=-T_n(X_1,\ldots,X_n)$.
\end{itemize}
Abbiamo inoltre che la distribuzione di $(X_ 1,\ldots,X_n)$ e del vettore $(-X_ 1,\ldots,-X_n)$ coincidono (ricordando che l'origine è il centro di simmetria). Si avrà dunque: 
\begin{eqnarray*}
\mathbb{E}[T_n] &=& \mathbb{E}[T_n(X_1,\ldots, X_n)]=\mathbb{E}[T_n(-X_1\ldots ,-X_n)] \\
&=& \mathbb{E}[-T_n(X_1,\ldots, X_n)] = -\mathbb{E}[T_n(X_1\ldots ,X_n)] \\
&=& -\mathbb{E}[T_n]
\end{eqnarray*}
perciò, in definitiva, $2\mathbb{E}[T_n]=0$ ovvero $\mathbb{E}[T_n]=0$. Quindi, nel caso di una distribuzione simmetrica, la media campionaria è uno stimatore non distorto del valore di aspettazione, del punto di simmetria e della media della popolazione (dato che tutti loro nel nostro caso coincidono). \\

\noindent \textbf{Esempio} Sia $(X_1,\ldots,X_n)$ un campione casuale di parametro $\theta\in \mathbb{R}$ fissato e con $\mu\coloneqq \mathbb{E}[X]$, $\sigma^2\coloneqq \var[X]$. Vogliamo provare a calcolare la distorsione di due stimatori ``classici'':
\begin{enumerate}
\item Scegliamo come stimatore la \textit{media campionaria} $\overline{X}_n\coloneqq \frac{1}{n}\sum_{i=1}^n X_i$. Allora $B_{\theta}[\overline{X}_n]=\mathbb{E}_{\theta}[\overline{X}_n]-\theta=\frac{1}{n}\sum_{i=1}^{n}\mathbb{E}_{\theta}[X_i]-\theta=\frac{1}{n}\cdot n\mathbb{E}_\theta[X]=\mu-\theta$. Dunque la distorsione è costante $\forall n\in \mathbb{N}$. In particolare è uno stimatore non distorto per il valore di aspettazione $\mu$. Possiamo calcolare facilmente anche la varianza di $\overline{X}_n$ che risulta essere $\frac{\sigma^2}{n}$. La media campionaria si rivela essere quindi un buon stimatore. (nota: nel calcolo della varianza stiamo trattando lo stimatore come una variabile casuale essa stessa, quindi più la varianza è piccola megliore è lo stimatore).
\item Prendiamo ora come stimatore la \textit{varianza campionaria}, data dalla variabile $S_n^2\coloneqq \frac{1}{n-1}\sum_{i=1}^n (X_i-\overline{X}_n)^2$. Allora: \\
 $\mathbb{E}[S_n^2]=\frac{1}{n-1}\sum_{i=1}^n \mathbb{E}[(X_i-\overline{X}_n)^2]=\frac{1}{n-1}\sum_{i=1}^n \mathbb{E}\big[\left((X_i-\mu)-(\overline{X}_n-\mu)\right)^2\big]=$
 
 
 $=\frac{1}{n-1}\left(\sum_{i=1}^n \mathbb{E}[(X_i-\mu)^2]-\sum_{i=1}^n\mathbb{E}[(\overline{X}_n-\mu)^2]\right)$
$=\frac{1}{n-1}\cdot (n-1)\sigma^2=\sigma^2.$ Perciò $S_n^2$ è uno stimatore non distorto di $\sigma^2$. Notiamo che lo stimatore $S_n^*\coloneqq \frac{1}{n}\sum_{i=1}^n (X_i-\overline{X}_n)^2$ avrebbe distorsione $-\frac{\sigma^2}{n}$, e dunque è peggiore della varianza campionaria, anche se è asintoticamente non distorto. \\ 
\end{enumerate}
Calcoleremo adesso la varianza della varianza campionaria. Assumiamo per il momento che il campione provenga da una distribuzione normale $N(\mu,\sigma^2)$. In tal caso mostriamo che $\frac{n-1}{\sigma^2}S_n^2 \sim \chi_{n-1}^2$ e dunque si avrà subito $\var[S_n^2]=\frac{2\sigma^4}{n-1}$. Infatti, $\frac{n-1}{\sigma^2}S_n^2=\sum_{i=1}^n \left(\frac{X_i-\overline{X}_n}{\sigma}\right)^2=\sum_{i=1}^n\left(\frac{X_i-\mu}{\sigma}-\frac{\overline{X}_n-\mu}{\sigma}\right)^2=\sum_{i=1}^n\left(\frac{X_i-\mu}{\sigma}\right)^2-n\left(\frac{\overline{X}_n-\mu}{\sigma}\right)^2=\sum_{i=1}^n\left(\frac{X_i-\mu}{\sigma}\right)^2-\left(\frac{\overline{X}_n-\mu}{\frac{\sigma}{\sqrt{n}}}\right)^2\Rightarrow \sum_{i=1}^n \left(\sim \chi_1^2\right)-\left(\sim \chi_1^2\right)\Rightarrow \frac{n-1}{\sigma^2}S_n^2 \sim \chi_{n-1}^2$, ove abbiamo usato il seguente teorema: \\
\\
\begin{teo}
Sia $(X_1,\ldots,X_n)$ un campione casuale ove la funzione generatrice di ciascuna $X_i$, $1\leq i\leq n$ è $M_{X}(t)$. Allora $M_{\overline{X}_n}(t)=(M_{X}(\frac{t}{n}))^n$.
\end{teo}
$\vspace{3 mm}$
per mostrare che $\overline{X}_n\sim N(\mu, \frac{\sigma^2}{n})$. Infatti: \\ 
$M_{\overline{X}_n}(t)=(M_{X}(\frac{t}{n}))^n=\left(e^{\mu\frac{t}{n}+\frac{\sigma^2 t^2}{2n^2}}\right)^n=e^{\mu t+\frac{\sigma^2}{n}\cdot\frac{t}{2}}$, da cui la tesi. \\

\begin{teo}
Sia $(X_1,\ldots,X_n)$ un campione casuale da una popolazione con distribuzione discreta o assolutamente continua dove la densità associata sia della forma $f(x,\theta)=C(x)D(\theta)\exp\{\sum_{m=1}^k A_m(\theta)B_m(x)\}$ con $k$ naturale positivo. Siano $T_1,\ldots,T_k$ statistiche definite $\forall 1\leq m\leq k$ da $T_m(X_1,\ldots,X_n)\coloneqq \sum_{i=1}^n B_m(X_i)$. Allora la distribuzione di $(T_1,\ldots,T_k)$ sarà ancora della forma esponenziale:
$$f_{(T_1,\ldots,T_k)}(\theta,t_1,\ldots,t_k)=C(t_1,\ldots,t_k)D(\theta)^n exp\bigg\{\sum_{i=1}^k A_m(\theta)t_m\bigg\}$$.
\end{teo}

\textit{Esempio.} Sia $(X_1,\ldots,X_n)$ un campione casuale. Supponiamo che $X\sim \bi(1,p)$. Allora la densità sarà discreta, ossia sarà $f(p,x)=\mathbb{P}[X=x]=\mathbbm{1}_{\{0,1\}}(x)(1-p)\exp\big\{x \log\big(\frac{p}{1-p}\big)\big\}$. Applicando il teorema otteniamo $T_1(X_1,\ldots,X_n)=$ $\sum_{i=1}^n B_1(X_i)=\sum_{i=1}^n X_i$ da cui si deduce subito che $T_1\sim \bi(n,p)$. In particolare possiamo scriverne la densità: $f_{T_1}(p,t_1)=\mathbbm{1}_{\{0,\ldots,n\}}(t_1)\binom{n}{t_1}(1-p)^n\exp\{t_1 \log\big(\frac{p}{1-p}\big)\}$. \\ 

\paragraph{Massimo campionario.} Sia \((X_1,\dotsc,X_n)\) un campione casuale da distribuzione uniforme \(\mathcal{U}([0,\theta])\). Essendo \(\theta\) il massimo valore che ciascuna variabile può assumere, uno stimatore plausibile
per \(\theta\) potrebbe essere il massimo campionario, cioè \(X_{(n)} = \max\lbrace X_1,\dotsc,X_n\rbrace\). Per il Teorema~\ref{dist:ordine}, ne conosciamo già la distribuzione:
\begin{equation*}
  F_X(x,\theta) = \frac{x}{\theta} \implies
  f_{X_{(n)}}(x,\theta) = \frac{n}{\theta^n}x^{n-1}\mathbb{I}_{[0,\theta)]}(x).
\end{equation*}
Per quanto riguarda la proprietà di distorsione del massimo campionario, poiché
\begin{align*}
  \mathbb{E}(X_{(n)}) &= \int_0^\theta \frac{n}{\theta^n}x^n\mathrm{d}x
                     = \frac{n}{n+1}\theta \\
  B_{\theta}(X_{(n)}) &= \frac{-\theta}{n+1} \ne 0,
\end{align*}
concludiamo che esso è stimatore distorto e asintoticamente non distorto
per \(\theta\).
Ne segue che è distorto, ma asintoticamente non distorto per $\theta$.
La sua varianza corrisponde a
\begin{equation*}
  \var[X_{(n)}]=\mathbb{E}[X_{(n)}^2]-\mathbb{E}[X_{(n)}]^2=\int_0^\theta \frac{n}{\theta^n}x^{n+1} dx -\left(\frac{n}{n+1}\theta\right)^2=\frac{n\theta^2}{(n+1)^2(n+2)}
  \xrightarrow[n\rightarrow \infty]{}0.
\end{equation*}
Perciò il massimo $X_{(n)}$ rimane in ogni caso uno stimatore affidabile. Osserviamo che possiamo tuttavia introdurre un nuovo stimatore che ci assicura la non distorsione, ovvero $T_n^*\coloneqq \frac{n+1}{n}T$, che possiede le proprietà cercate.

\begin{dfn}[Consistenza]
  Sia \(X\) una variabile casuale avente distribuzione \(F(x,\theta)\).
  Sia \((X_1,\dotsc,X_n)\) un campione casuale dalla distribuzione di \(X\) e
  \(T_n\) una statistica. Diciamo che \(T_n\) è uno stimatore \emph{consistente}
  per \(\theta \in \Theta\) se %\(T_n \xrightarrow[]{\mathbb{P}} \theta\):
  \(T_n\) converge in probabilità a \(\theta\).
\end{dfn}

\noindent\textit{Esempio.} Sia $(X_1,\ldots,X_n)$ un campione casuale, ove $X\in \mathcal{L}^2(\mathbb{R)}$. Indichiamo come al solito media e varianza rispettivamente con $\mu$ e $\sigma^2$. Allora abbiamo:
\begin{enumerate}
\item La media campionaria $\overline{X}_n\coloneqq \frac{1}{n}\sum_{i=1}^n X_i\xrightarrow[n\rightarrow \infty]{\mathbb{P}} \mu$, grazie alla legge debole dei grandi numeri poiché $\lim_{n\rightarrow +\infty} \mathbb{P}[(\overline{X}_n-\mu)>\varepsilon ]=0$, $\forall \varepsilon >0$.
\item Consideriamo adesso la varianza campionaria $$S_n^2\coloneqq \frac{1}{n-1}\sum_{i=1}^n (X_i-\overline{X}_n)^2=\frac{n}{n-1}\left(\frac{1}{n}\sum_{i=1}^n X_i^2-\overline{X}_n^2\right)$$. Abbiamo ora i seguenti tre termini:
\begin{itemize}
\item $\lim_{n\rightarrow +\infty} \frac{n}{n-1}=1$, un semplice limite;
\item $\frac{1}{n}\sum_{i=1}^n X_i^2 \xrightarrow[n\rightarrow \infty]{\mathbb{P}} \mathbb{E}[X^2]$, ancora grazie alla legge debole dei grandi numeri e al fatto che $X^2$ rimane ancora sommabile;
\item $\overline{X}_n^2 \xrightarrow[n\rightarrow \infty]{\mathbb{P}} \mu^2=\mathbb{E}[X]^2$ grazie al Teorema 4 sulla convergenza.
\end{itemize}
Ne segue quindi che $S_n^2 \xrightarrow[n\rightarrow \infty]{\mathbb{P}} \sigma^2$, sempre per i teoremi sulla convergenza di somma, prodotto e prodotto per costanti di variabili casuali.
\item Consideriamo ancora il campione casuale distribuito uniformemente $\uni([0,\theta])$ con stimatore $T_n(X_1\ldots,X_n)\coloneqq X_{(n)}$. Troviamo che anch'esso è consistente per la stima del massimo. Infatti, $\mathbb{P}[|T_n-\theta|>\varepsilon]=\mathbb{P}[\theta-T_n>\varepsilon]=\mathbb{P}[X_{(n)}\leq \theta-\varepsilon]=F_{X_{(n)}}(\theta -\varepsilon)=\left(1-\frac{\varepsilon}{\theta}\right)^n \xrightarrow[n\rightarrow \infty]{} 0$. Allo stesso modo si può verificare che anche $T_n^*$ è consistente per $\theta$.
\end{enumerate}
\textbf{Definizione.} Sia $(X_1,\ldots,X_n)$ un campione casuale e $T_n: \mathfrak{X}\longrightarrow\mathcal{Y}_{T_n}$ una statistica (stimatore). Vi sia inoltre una funzione di parametri $a: \Theta\longrightarrow\mathcal{Y}_{\Theta}$. Allora la funzione non negativa $\lo: \left(\mathcal{Y}_{T_n}\cup\mathcal{Y}_{\Theta}\right)\times\mathcal{Y}_{\Theta}\longrightarrow \mathbb{R}_{\geq 0}$ viene detta \textit{Funzione di Perdita} se soddisfa alle seguenti condizioni:
\begin{enumerate}[noitemsep]
\item $\lo\big(a(\theta),a(\theta)\big)=0$, $\forall \theta\in \Theta$;
\item Per ogni $T_n\in \mathcal{T}$, esiste una funzione $\ri: Y_{T_n}\times Y_{\Theta}\longrightarrow \mathbb{R}$, detta \textit{Funzione di Rischio}, tale che $\ri\big(T_n,a(\theta)\big)=\mathbb{E}_{\theta}[\lo\big(T_n,a(\theta)\big)]$, $\forall \theta\in \Theta$. 
\end{enumerate}
\textit{Osservazione.} La funzione di perdita può essere pensata come una misura della discrepanza tra l'azione $T_n$ e lo stato della natura $a(\theta)$. \\ \\
\textbf{Definizione.} Possiamo già definire due tipologie di funzioni di perdita che spesso vengono utilizzate in statistica: 
\begin{enumerate}[noitemsep]
\item $\lo_1\big(T_n,a(\theta)\big)\coloneqq |T_n-a(\theta)|$, chiamata \textit{Errore assoluto};
\item $\lo_2\big(T_n,a(\theta)\big)\coloneqq \big(T_n-a(\theta)\big)^2$. Essa ammette anche come possibile funzione di rischio $\ri_2\big(T_n,a(\theta)\big)\coloneqq \mathbb{E}_{\theta} \left[(T_n-a(\theta))^2\right]$; se tuttavia $a=\id_{\Theta}$, allora la funzione $\mse_{\theta}(T_n)\coloneqq \ri_2\big(T_n,\theta\big)$ prende il nome di \textit{Mean Square Error} (oppure \textit{Errore Quadratico Medio}).
\end{enumerate}
\textit{Osservazione.} Semplicemente aggiungendo e sottraendo il valore $\mathbb{E}[T_n]^2$ si ottiene subito la seguente uguaglianza: $\mse_{\theta}(T_n)=\var_\theta [T_n]+B_\theta [T_n]^2$.
\begin{teo}
Sia $T_n$ uno stimatore di $\theta$ (non necessariamente non distorto). Allora si ha che $\lim_{n\rightarrow +\infty} \mse_\theta (T_n)=0$ è condizione sufficiente (ma non necessaria) per la consistenza di $T_n.$
\end{teo}
\begin{proof} Si ha infatti la seguente semplice catena di diseguaglianze: 
\begin{eqnarray*}
\mathbb{P}[|T_n-\theta|>\varepsilon] &=& \int_{|T_n-\theta|>\varepsilon} f_{T_n}(\theta,t_n)dt_n \\ 
&<& \int_{|T_n-\theta|>\varepsilon} \frac{(t_n-\theta)^2}{\varepsilon^2}f_{T_n}(\theta,t_n)dt_n < \frac{1}{\varepsilon^2}\mse_\theta (T_n).
\end{eqnarray*}
\end{proof}
