%%%%lezione 18 febbraio%%%%
\chapter{Introduzione}
In questa prima sezione vengono presentati i richiami di teoria della probabilità, affrontati nelle primissime lezioni del corso.

\section{Funzione generatrice dei momenti}
Lezione del 18/02, ultima modifica 09/04, Andrea Gadotti

\begin{dfn}[fgm]
Sia \(X\) una variabile casuale, discreta o assolutamente continua.
Se esiste $t_0 \in \mathbb{R}_{+}$ tale per cui \(\mathbb{E}(e^{tX}) < +\infty\) per ogni \(t \in (-t_0 , t_0)\), chiameremo la funzione 
\begin{equation}
  M_X := \mathbb{E}(e^{tX})
\end{equation}
\emph{funzione generatrice dei momenti} di \(X\).
\end{dfn}

\subsection{Esempi}
\begin{description}
\item[Bernoulli] Sia \(X \sim b(1,p)\), con \(p \in (0,1)\). Si ha:
\begin{equation*}
  M_X (t) = \mathbb{E}(e^{tX})
          = \sum_{x=0}^1 e^{tx} \mathbb{P}(X=x)
          = \sum_{x=0}^1 e^{tx} p^x (1-p)^{1-x}
          = p e^t + (1-p).
\end{equation*}

\item[Poisson] Sia \(X \sim \mathcal{P}(\lambda)\), con \(\lambda \in \mathbb{R}_+\). Si ha:
\begin{equation*}
  M_X (t) = \mathbb{E}(e^{tX}) 
          = \sum_{x=0}^{+\infty} e^{tx} \frac{e^{-\lambda} \lambda^x}{x!}
          = e^{\lambda (e^t -1)}.
\end{equation*}

\item[Gamma] Sia \(X \sim \mathcal{G}(\alpha, \beta)\): allora, la densità di \(X\) è data da 
\begin{equation*}
  f_X (x; \alpha, \beta) := \frac{1}{\Gamma (\alpha) \beta^{\alpha}} x^{\alpha -1} e^{-\frac{1}{\beta} x} \mathbb{I}_{\mathbb{R}_+}(x), \quad
  \alpha, \beta \in \mathbb{R}_+
\end{equation*}
dove \(\Gamma{}\) indica la funzione di Eulero
\begin{equation*}
  \Gamma(\alpha) := \int_0^{+\infty} \! x^{\alpha -1} e^{-x} \mathrm{d}x,
\end{equation*}
la quale è tale che \(\Gamma(\alpha) = (\alpha - 1)!\) se \(\alpha \in \mathbb{N}\).

La generatrice dei momenti di \(X\) risulta essere: 
\begin{align*}
  M_X (t)	&= \mathbb{E}(e^{tX})
           = \int_0^{+\infty} \! e^{tx} \frac{1}{\Gamma (\alpha) 
            \beta^{\alpha}} x^{\alpha -1} e^{-\frac{1}{\beta} x} \mathrm{d}x \\
          &= \frac{1}{\Gamma(\alpha)\beta^{\alpha}}
             \int_0^{+\infty} e^{-\tau}
             \frac{\beta^{\alpha-1}}{(1-\beta t)^{\alpha-1}}
             \tau^{\alpha-1}
             \frac{\beta}{1-\beta t}\mathrm{d}\tau,\qquad \tau := x\left(\frac{1}{\beta} - t\right) \\
          &= \frac{1}{(1-\beta t)^{\alpha}}
             \frac{\int_0^{+\infty} \! \tau^{\alpha-1} e^{-\tau} \mathrm{d}\tau}%
             {\Gamma(\alpha)}
           = \frac{1}{(1-\beta t)^{\alpha}}, \qquad t < \frac{1}{\beta}.
\end{align*}
\end{description}

\section{Momenti di una variabile casuale}

\begin{dfn}
Se una variabile casuale ammette fgm derivabile infinite volte in un intorno di \(t=0\) e se tutti i suoi momenti sono finiti, allora definiamo il \emph{momento di ordine \(s\) non centrato}: 
\begin{equation}
  \mu'_s := \mathbb{E}(X^s) = \restr{\frac{d^s}{dt^s} M_X(t)}{t=0}.
\end{equation} 
Il \emph{momento di ordine \(s\) centrato} in \(a \in \mathbb{R}\) è definito come: 
\begin{equation}
  \mu_s (a) := \mathbbm{E} ((X-a)^s),
\end{equation}
ovvero $\mu'_s = \mu_s (0)$.
%E' chiaro che $\mu'_1 = \mathbbm{E} (X)$.
%Chiameremo infine momento di ordine $s$ centrato (senza specificare altro, intenderemo centrato in $\mu'_1$):

%$$\mu_s := \mathbbm{E} ((X-\mu'_1)^s)$$
\end{dfn}

\begin{oss}
  E\' chiaro che \(\mu'_1 = \mathbb{E}(X)\). Se non viene specificato altro, chiameremo \emph{momento di ordine \(s\) centrato} la quantità
  \begin{equation}
    \mu_s = \mathbb{E}((X-\mu'_1)^s).
  \end{equation}
\end{oss}

\begin{teo}
Dall'ultima definizione si vede facilmente (sviluppando l'elevamento a potenza) che vale la seguente relazione tra momenti centrati e non:
$$\mu_s = \mathbbm{E} ((X-\mu'_1)^s) = \displaystyle\sum_{m=0}^s (-1)^m \binom{s}{m} \mu'_{s-m} (\mu'_1)^m$$
\end{teo}
Osserviamo che $\mu_2 = \mathbbm{E} ((X-\mu'_1)^2) = Var(X) = \mathbbm{E} (X^2) - (\mathbbm{E} (X))^2 = \mu'_2 - (\mu'_1)^2$


\begin{teo}
Date due (o più) v.c. $X$ e $Y$ indipendenti aventi $f$ densità / $f$ massa $f_X$ e $f_Y$ e fgm $M_X(t)$ e $M_Y(t)$ rispettivamente, allora si ha

$$M_{X+Y}(t) = M_X(t) M_Y(t)$$
\end{teo}

\begin{teo}
Siano $X$ e $Y$ v.c. con funzioni di ripartizione $F_X(x)$ e $F_Y(y)$ rispettivamente. Siano $M_X(t)$ e $M_Y(t)$ le fgm di $X$ e $Y$. Se $M_X(t) = M_Y(t)$ per ogni $t$ in un intorno dell'origine, allora 

$$X \stackrel{d}{=} Y$$
\end{teo}

\begin{oss}
Il teorema appena visto ci dice sostanzialmente che, se esiste, la fgm caratterizza la distribuzione della corrispondente v.c.
\end{oss}
\noindent \textbf{Esempio}
Siano $(X_1,...,X_n)$ risultati della replicazione di un esperimento casuale dicotomico $(X_i \sim b(1,p))$. Vogliamo trovare la distribuzione di $S_n := \displaystyle\sum_{i=1}^n X_i$. Calcoliamo quindi la sua fgm:

$$M_{S_n}(t) = \mathbbm{E} \left( {e^{t S_n}} \right)  = \mathbbm{E} \left( {e^{t \sum_{i=1}^n X_i}} \right) $$
$$\stackrel{TEO 2}
{=} \displaystyle\prod_{i=1}^n \mathbbm{E}({e^{tX_i}}) = \displaystyle\prod_{i=1}^n M_{X_i}(t) = \displaystyle\prod_{i=1}^n (p e^t + (1-p)) = (p e^t + (1-p))^n$$ ovvero $S_n$ è distribuita come $b(n,p)$ per il Teorema 2.

\noindent \textbf{Esercizio}
Ripetere il calcolo precedente supponendo $X_i \sim P(\lambda), \forall i$.

\subsection{Famiglia Esponenziale a $k$ parametri}
Una famiglia di $f$ densità / $f$ massa è detta essere una Famiglia Esponenziale a $k$ parametri $\theta_1,...,\theta_k$ se la corrispondente $f$ densità / $f$ massa (che è indicizzata da $\theta_1,...,\theta_k$) può essere scritta come

$$f_X(x;\theta) = C^*(x) D^*(\theta) \exp \lbrace \displaystyle\sum_{m=1}^k A_m(\theta) B_m (x) \rbrace$$

dove $C^*(x)$ è una funzione della sola $x$, $D^*(\theta)$ è una funzione del solo $\theta$, $A_m(\theta)$ è una funzione del solo $\theta$ e $B_m(x)$ è una funzione della sola $x$.

\noindent \textbf{Esempi}
\begin{enumerate}
\item 
$X \sim G(\alpha, \beta) \Longrightarrow f_X(x; \alpha, \beta) = \frac{1}{\Gamma(\alpha) \beta^\alpha} x^{\alpha -1} e^{- \frac{1}{\beta} x} \mathbbm{1}_{\mathbbm{R}^+}(x)$, $\alpha >0$, $\beta >0$
$\mathbbm{1}_{\mathbbm{R}^+}$ è detto supporto della distribuzione.
Quindi possiamo riscrivere $f_X(x; \alpha, \beta)$ come
$$f_X(x; \alpha, \beta) = \frac{1}{\Gamma(\alpha) \beta^\alpha} \mathbbm{1}_{\mathbbm{R}^+}(x) exp((\alpha -1) ln(x) - \frac{1}{\beta} x)$$
e quindi ponendo $D^*(\alpha, \beta) := \frac{1}{\Gamma(\alpha) \beta^\alpha}$, $C^*(x) := \mathbbm{1}_{\mathbbm{R}^+}(x)$, $A_1(\alpha, \beta) := (\alpha -1)$, $B_1(x) := ln(x)$, $A_2(\alpha, \beta) := - \frac{1}{\beta}$ e $B_2(x) := x$, otteniamo $G(\alpha, \beta)$ come famiglia esponenziale con $k=2$.

\item $X \sim b(n,p) \Longrightarrow f_X(x; n,p) = \binom{n}{x} p^x (1-p)^{n-x} \mathbbm{1}_{\lbrace 0,1,...,n \rbrace}(x)$ con $n \in \mathbbm{N}$ noto.
Quindi possiamo riscrivere $f_X(x; n,p)$ come
$$f_X(x; n,p) = \binom{n}{x} \mathbbm{1}_{\lbrace 0,1,...,n \rbrace}(x) (1-p)^n exp(x \ln(\frac{p}{1-p}))$$ con $\frac{p}{1-p}$ detto odd ratio o parametro naturale della famiglia esponenziale.\\
Quindi ponendo $D^*(p) := (1-p)^n$, $C^*(x) := \binom{n}{x} \mathbbm{1}_{\lbrace 0,1,...,n \rbrace}(x)$, $A_1(p) := ln(\frac{p}{1-p})$, $B_1(x) := x$, otteniamo $b(n,p)$ come famiglia esponenziale con $k=1$.

\item X vc con $f_X(x,\vartheta)=\frac{e^{1-x/\vartheta}}{\vartheta}\mathbbm{1}_{(\vartheta,\infty)}(x)$: la distribuzione di X non appartiene a famiglia esponenziale.
Il fatto che il supporto di $f_X$ dipenda dal parametro $\vartheta$ NON permette a $f_X$ di appartenere ud una famiglia esponenziale!
\end{enumerate}

\begin{oss}
Le famiglie di esponenziali hanno interessanti proprietà matematiche (proprietà di regolarità).

Dal punto di vista statistico, ciò si traduce in un'interessante conseguenza: tutta l'informazione contenuta nei dati a disposizione $(X_1,...,X_n)$ relativa alla funzione $f_X (x; \theta)$ può essere sintetizzata attraverso $k$ quantità (funzioni di $(X_1,...,X_n)$) che potranno essere impiegate per costruire procedure inferenziali (stima, test per la verifica di ipotesi) riguardanti il parametro $\theta$.\\
Ovvero, l'appartenenza ad una famiglia esponenziale permette una riduzione dei dati $(X_1$, ... ,$X_n)$ via $B_m$.\end{oss}