%%%%lezione 20 maggio%%%%

Lezione del 20/05, ultima modifica 21/06, Andrea Gadotti

\section{Stimatori di massima verosimiglianza}

\begin{prp}
  Uno stimatore di massima verosimiglianza per il parametro \(\theta\) è
  funzione di una statistica sufficiente per \(\theta\).
\end{prp}
\begin{proof}
  Assumendo l'esistenza di una statistica sufficiente \(T_n\) per \(\theta\),
  è possibile fattorizzare secondo Neyman la funzione di verosimiglianza
  ottenendo le uguaglianze
  \begin{align*}
    L(\theta;\mathbf{x}) &= h(x)g(\theta,t_n), \\
    l(\theta;\mathbf{x}) &= \ln h(x) + \ln g(\theta,t_n).
  \end{align*}
  Ora, l'equazione di stima per lo stimatore di massima verosimiglianza
  data da
  \begin{equation*}
    \frac{d}{d\theta}l(\theta;\mathbf{x}) =
    \frac{d}{d\theta}\ln g(\theta,t_n) = 0
  \end{equation*}
  fornisce una prova dell'indipendenza dello stimatore di massima verosimiglianza rispetto alla determinazione campionaria e al parametro
  da stimare, mantenendo dipendenza unicamente dalla statistica sufficiente.
\end{proof}

\begin{prp}[Proprietà di invarianza]
  Sia \(g(\theta)\) una funzione reale del parametro \(\theta\),
  indicizzante la famiglia parametrica di distribuzioni \(\mathcal{F}_\theta\)
  da cui proviene il campione casuale \(\mathbf{X}\).
  Assunto \(\hat{\theta} = \hat{\theta}(\mathbf{X})\) essere il relativo
  stimatore di massima verosimiglianza, lo stimatore di massima
  verosimiglianza di \(g(\theta)\) è dato da
  \begin{equation}
    \hat{\eta} = g(\hat{\theta}).
  \end{equation}  
\end{prp}

Gli stimatori di massima verosimiglianza godono delle seguenti proprietà:

\begin{enumerate}
\item[(3)] \textbf{Efficienza (per n finito):} nel caso in cui la distribuzione provenga da una famiglia regolare, se esiste uno stimatore non distorto $T_n$ di $\theta$ la cui varianza raggiunge il limite inferiore di Rao-Cramer, tale stimatore coincide con lo stimatore di MV di $\theta$. (nota: in generale gli stimatori di MV sono distorti)
\item[(4)] \textbf{Consistenza:} gli stimatori di MV sono consistenti
\begin{itemize}
\item in senso forte (ovvero sono \emph{quadraticamente consistenti}):
$$\mse_{\theta}(T_n)=\var_{\theta}(T_n)+B_{\theta}(T_n) \stackrel{n \rightarrow \infty}{\longrightarrow} 0$$
\item in senso debole (ovvero sono \emph{semplicemente consistenti})
$$\mathbb{P}(|\hat{\theta}_n-\theta|>\varepsilon) \stackrel{n \rightarrow \infty}{\longrightarrow} 0$$
\end{itemize}
\item[(5)] \textbf{Efficienza (per $n \rightarrow \infty$):} sotto condizioni molto generali di regolarità, lo stimatore di MV $\hat{\theta}_n$ è asintoticamente efficiente, ovvero $\hat{\theta}_n$ è tale che:
\begin{enumerate}
\item[a)] $\lim_{n \rightarrow \infty} \mathbb{E}_{\theta}(\hat{\theta}_n)=\theta \; \; \forall \theta \in \Theta$
\item[b)] $\lim_{n \rightarrow \infty} \var_{\theta}(\hat{\theta}_n)=\frac{1}{I(\theta)} \; \; \forall \theta \in \Theta$
\end{enumerate}
\item[(6)] \textbf{Distribuzione asintotica:} sempre sotto alcune ipotesi di regolarità, lo stimatore di massima verosimiglianza per $n$ grande ha distribuzione normale, ovvero
$$\hat{\theta}_n \stackrel{a}{\sim} N \left( \theta, \frac{1}{I(\theta)} \right)$$
Questo risultato risulta molto utile in ambito inferenziale:
\begin{esempio}
$$IC_{\theta}(\-\alpha)=\left[ \hat{\theta}_n - z_{1-\alpha/2} \frac{1}{\sqrt{I(\hat{\theta}_n)}}, \; \; \hat{\theta}_n + z_{1-\alpha/2} \frac{1}{\sqrt{I(\hat{\theta}_n)}} \right]$$
dove possiamo anche sfruttare la proprietà $I(\hat{\theta}_n)= n I_1(\hat{\theta}_n)$.
\end{esempio}
\begin{oss}
Sia $g(\theta)$ una funzione continua di $\theta$ e derivabile in $\theta_0$ tale che $g'(\theta_0) \neq 0$. Allora
$$\sqrt{n} (g(\hat{\theta}_n - g(\theta_0)) \stackrel{d}{\longrightarrow} N \left( 0, \frac{(g'(\theta_o))^2}{I(\theta_0)} \right)$$
dove notiamo che, per la proprietà (1), $g(\hat{\theta}_n)$ è lo stimatore di MV per $g(\theta)$.\\
La dimostrazione è immediata usando il $\Delta$-method.
\\
\\
\end{oss}
\end{enumerate}